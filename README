Steps to run the service:
1. start mongodb, create db: webcontents
2. prepare.sh               #install prerequsites, not a complete list
3. nodemon db_server.js     #mongo db service (port 4040)
4. python phantom_manager.py log_dir phanton_worker.js_path # this manager starts a server (port 8082) to receive web contents crawl/execution tasks


Steps to train a website:
1. modify spider.py to give a domain name
2. scrapy runspider spider.py > evaluation/tmp/doman.txt
3. processURL.py evaluation/tmp/doman.txt evaluation/urls/
5.  python send_get_req.py evaluation/urls/domain_train.txt
	python send_get_req.py evaluation/urls/domain_test.txt

==============
6. python processScript.py evaluation/urls/domain_train.txt # get script
	python processScript.py evaluation/urls/domain_test.txt # get script
8. train: mkdir tmp/domain; python template.py evaluation/urls/domain_train.txt tmp/domain
9. test:  python handler.py domain evaluation/urls/domain_test.txt 



Some commands:
notes: ps aux | grep "python" | grep -v "grep"| awk '{print $2}'  | xargs kill -9
scrapy runspider spider.py
db.trees.createIndex({domain:1, key:1},{unique:true})
